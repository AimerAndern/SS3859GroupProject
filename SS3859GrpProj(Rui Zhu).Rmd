---
title: "SS3859 Group Project"
author: "Rui Zhu"
output:
  word_document: default
  html_notebook: default
  pdf_document: default
  html_document:
    df_print: paged
---


\newpage
# 1. Calculate p-value for each predictor
```{r}
data <- read.csv(file = 'PM2.5.csv')
data=na.omit(data)
set.seed(3859)
data$cbwd <- as.factor(data$cbwd)
data <- data[-c(1)]

index <- sample(1:nrow(data), 2000)
df=data[index, ]
```

# 2. Some hypothese that which predictors are not significant 
 * By observing the summary table of the full model, we make the null hypothesis that the Is is not significantly important to explain this model.
```{r}
#reduced model without Is
model2=lm(pm2.5 ~ DEWP+TEMP+PRES+Iws+cbwd+Ir, data=df)
summary(model2)
anova(model1,model2)
```

 * The large f statistics value in anova indicates there is no significant difference between the two models. Therefore, we fail to reject the null hypothesis that Is is significantly important to explain the model.
 

 
 
 
# 3. Test for interactions(2 way interaction)
```{r}
model4=lm(pm2.5 ~ DEWP+TEMP+PRES+cbwd+Iws+Ir+Is+I(DEWP*TEMP)+I(DEWP*PRES)+I(DEWP*Iws)+I(DEWP*Ir)+I(TEMP*PRES)+I(TEMP*Iws)+I(TEMP*Ir)+I(PRES*Iws)+I(PRES*Ir)+I(Iws*Ir)+I(Is*TEMP)+I(Is*DEWP)+I(Is*PRES)+I(Is*Iws)+I(Is*Ir), data=df)


summary(model4)
```
 * By observing the p-value of each predictior, we make the null hypothesis that DEWP * TEMP, DEWP * Ir, TEMP * Ir and PRES * Ir, Is * TEMP, Is * DEWP and Is * PRES are not significantly important to explain this model. 
```{r}
#reduced model without DEWP * TEMP, DEWP * Ir, TEMP * Ir and PRES * Ir, Is * TEMP, Is * DEWP and Is * PRES
model5=lm(pm2.5 ~ DEWP+TEMP+PRES+cbwd+Iws+Ir+Is+I(DEWP*PRES)+I(DEWP*Iws)+I(TEMP*PRES)+I(TEMP*Iws)+I(PRES*Iws)+I(Iws*Ir)+I(Is*Iws)+I(Is*Ir), data=df)
summary(model5)
anova(model4,model5)
```
 * The large p-value in anova indicates there is no significant difference between the two models.
 
 
# 4. Variable selection (which variables to keep, based on previous results and AIC, BIC or PRESS test)
```{r}
nullfit <- lm(pm2.5~1,data=df)
stepAppro_aic = step(nullfit,
                     scope = pm2.5 ~ DEWP+TEMP+PRES+cbwd+Iws+Ir+Is+I(DEWP*TEMP)+I(DEWP*PRES)+I(DEWP*Iws)+I(DEWP*Ir)+I(TEMP*PRES)+I(TEMP*Iws)+I(TEMP*Ir)+I(PRES*Iws)+I(PRES*Ir)+I(Iws*Ir)+I(Is*TEMP)+I(Is*DEWP)+I(Is*PRES)+I(Is*Iws)+I(Is*Ir),
                     direction = "forward",
                     trace = 0)

stepAppro_bic <- step(model4,
                      direction = "backward",
                      k=log(nrow(df)),
                      trace=FALSE)
stepAppro_aic
stepAppro_bic
anova(stepAppro_aic,stepAppro_bic)
library(asbio)
#model selected by AIC
press(lm(formula = pm2.5 ~ Iws + I(TEMP * PRES) + I(DEWP * PRES) + 
    cbwd + DEWP + I(PRES * Ir) + PRES + I(DEWP * Iws) + I(TEMP * 
    Iws) + I(PRES * Iws) + I(Iws * Ir) + I(Is * TEMP) + I(Is * 
    Iws) + I(Is * PRES) + TEMP, data = df)
)
#model selected by BIC
press(lm(formula = pm2.5 ~ DEWP + PRES + cbwd + Iws + I(DEWP * PRES) + 
    I(DEWP * Iws) + I(TEMP * PRES) + I(TEMP * Iws) + I(PRES * 
    Ir) + I(Is * PRES) + I(Is * Iws), data = df))
```
 * The PRESS statistic indicates that model selected by AIC is more preferred in this case. However, PRESS might not be approprite although the dataset is reduced already.



# 5. Model diagnostics on one well-fit model
```{r}
library(lmtest)
#we will be using the model selected by AIC in later learning
modelX=lm(formula = pm2.5 ~ Iws + I(TEMP * PRES) + I(DEWP * PRES) + 
    cbwd + DEWP + I(PRES * Ir) + PRES + I(DEWP * Iws) + I(TEMP * 
    Iws) + I(PRES * Iws) + I(Iws * Ir) + I(Is * TEMP) + I(Is * 
    Iws) + I(Is * PRES) + TEMP, data = df)
plot(fitted(modelX), resid(modelX),
     col = "blue", pch = 10,
     xlab = "fitted value",
     ylab = "residual",
     cex=1,
     main = "residual plot")
qqnorm(resid(modelX), col = "grey",pch=20,cex=2)
qqline(resid(modelX))
loggedModelX=lm(formula = log(pm2.5) ~ Iws + I(TEMP * PRES) + I(DEWP * PRES) + 
    cbwd + DEWP + I(PRES * Ir) + PRES + I(DEWP * Iws) + I(TEMP * 
    Iws) + I(PRES * Iws) + I(Iws * Ir) + TEMP, data = df)
plot(fitted(loggedModelX), resid(loggedModelX),
     col = "blue", pch = 10,
     xlab = "fitted value",
     ylab = "residual",
     cex=1,
     main = "residual plot")
bptest(modelX)
shapiro.test(resid(modelX))

timeset=c()
residset=c()
i=1
while (i<nrow(df)){
  timeset=append(timeset, as.Date(paste(df[i,"month"],df[i,"day"],df[i,"year"],sep="/"), "%m/%d/%Y"))
  residset=append(residset, df[i, "pm2.5"]-predict(modelX, df[i,]))
  i=i+1
}
plot(timeset, residset,
     col = "blue", pch = 10,
     xlab = "time",
     ylab = "residual",
     cex=1,
     main = "residual plot")
length(timeset)

```
  * Assumptions:
  
    * Linearity: The residuals distribute systematically and do not exhibit a mean of 
    zero. The linearity assumption is violated.
  
    * Equal Variance: The small p-value of the BP test indicates that the  
    variance assumption is violated.
    
    * Normality Assumption: The small p-value of SW test indicates that the normality
    assumption is violated. However, the logged model might hold the normality assumption.
    
    * Independence Assumption: The residual plot against time, the value of random errors 
    are independent. The normality assumption holds.



# 6. Check for unusual observations (Cook's distance for outlier using model )


```{r}
# Outlier check
outlier_original <- df[which(abs(rstandard(model1))>2),]
nrow(outlier_original)

outlier_improved <- df[which(abs(rstandard(modelX))>2),]
nrow(outlier_improved)

# About 5% of the data are outliers

# Now use Cook's distance to check for influential points
df[cooks.distance(modelX) > 4 / length(cooks.distance(modelX)),]

# Despite there are only 95 outliers, 1151 of the data have heavy influence on the model.
# We may still need to modify the model
```


# 7. Attempt transformations to model
```{r}
library(MASS)

attach(df)
boxcox(pm2.5 ~ Iws + I(TEMP * PRES) + I(DEWP * PRES) + 
    cbwd + DEWP + I(PRES * Ir) + PRES + I(DEWP * Iws) + I(TEMP * 
    Iws) + I(PRES * Iws) + I(Iws * Ir) + I(Is * TEMP) + I(Is * 
    Iws) + I(Is * PRES) + TEMP, lambda = seq(-0 ,0.2, length = 20))
detach(df)
# take lambda = 0.11, we can try both log transformation and (y^lambda-1)/lambda
lambda = 0.11
modelX_BC_transf <- lm((pm2.5^lambda-1)/lambda ~ Iws + I(TEMP * PRES) + I(DEWP * PRES) + 
    cbwd + DEWP + I(PRES * Ir) + PRES + I(DEWP * Iws) + I(TEMP * 
    Iws) + I(PRES * Iws) + I(Iws * Ir) + I(Is * TEMP) + I(Is * 
    Iws) + I(Is * PRES) + TEMP, data = df)
modelX_BC_transf
AIC(modelX_BC_transf)
AIC(modelX)


# Repeat step 5
plot(fitted(modelX_BC_transf), resid(modelX_BC_transf),
     col = "blue", pch = 10,
     xlab = "fitted value",
     ylab = "residual",
     cex=1,
     main = "residual plot")
qqnorm(resid(modelX_BC_transf), col = "grey",pch=20,cex=2)
qqline(resid(modelX_BC_transf))
bptest(modelX)
shapiro.test(resid(modelX))

# Compare to the previous model, normality significantly improved. The P-value for both BP test and Shapiro test are still small, meaning that the assumptions are not met.
```
We will use the updated model, modelX_BC_transf next

# 8. Check if predictors have multicollinearity, using VIF

```{r}
vif(modelX_BC_transf)
vif(model1)
# By comparing the results of the complicated model and the simplest model, we can see that although the VIF of some predictors from the more complicated model is high, the model still fits the data better. 
# This is a tradeoff between more predictors and high VIF
# The high VIF is due to interactions
# We will trust the model selected by AIC test instead






















```


























